# choose which model to use {tf : Transformer (BERT-like) / snrm : SNRM}
model: 'snrm'
vocab_size: 30522
batch_size: 64
lr: 0.0001
sparse_dimensions: 10000
tf:
  hidden_size: 256
  num_of_layers: 2
  num_attention_heads: 4
  input_length_limit: 150
  # the method that the hidden states over all sequence steps are being aggregated {CLS, AVG, MAX}
  pooling_method: 'CLS'
snrm:
  hidden_sizes: '200' # for multiple layers add '-' between hidden sizes  eg. ('100-400-200')
  n: 5
  dropout_p: 0.2
debug: False
top_results: 1000
max_candidates_per_posting_list: -1
num_epochs: 100
num_of_workers_index: 7
l1_scalar: 0.0
num_of_decimals: 5
balance_scalar: 0.0
disable_cuda: False
dataset_path: 'data/msmarco/'
embedding: 'glove'
glove_embedding_path: 'data/embeddings/glove.6B.300d.p'
glove_word2idx_path: 'data/embeddings/glove.6B.300d_word2idx_dict.p'
query_file_val: '${dataset_path}/msmarco-test2019-queries_43.${embedding}.tsv'
docs_file_val: '${dataset_path}/msmarco-passagetest2019-top1000_43.${embedding}.tsv'
qrels_val: '${dataset_path}/2019qrels-pass_filtered_ms_marco.txt'
q_docs_file_val: '${dataset_path}/msmarco-passagetest2019-top1000.tsv.sorted_cut'
qrels_dev: '${dataset_path}/qrels.dev.small.tsv'
q_docs_file_dev: '${dataset_path}/top1000.dev.sorted'
docs_file_train: '${dataset_path}/collection.${embedding}.tsv'
query_file_train: '${dataset_path}/queries.train.${embedding}.tsv'
triplets_file_train: '${dataset_path}qidpidtriples.train.full.tsv'

# debug_str = '' if not debug else '.debug'
# triplets_fname = f'{dataset_path}/qidpidtriples.train.full{debug_str}.tsv'
# documents_path = f'{dataset_path}/collection.tokenized.tsv'
# queries_path = f'{dataset_path}/queries.train.tokenized.tsv'
