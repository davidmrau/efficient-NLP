# choose which model to use {tf : Transformer (BERT-like) / snrm : SNRM}
model: 'snrm'
vocab_size: 30522
batch_size: 64
lr: 0.0001
sparse_dimensions: 10000
tf:
  hidden_size: 256
  num_of_layers: 2
  num_attention_heads: 4
  input_length_limit: 150
  # the method that the hidden states over all sequence steps are being aggregated {CLS, AVG, MAX}
  pooling_method: 'CLS'
snrm:
  hidden_sizes: '200' # for multiple layers add '-' between hidden sizes  eg. ('100-400-200')
  n: 5
  dropout_p: 0.2
debug: False
top_results: 1000
max_candidates_per_posting_list: -1
num_epochs: 100
num_of_workers_index: 7
l1_scalar: 0.0
num_of_decimals: 5
balance_scalar: 1.0
disable_cuda: False
dataset_path: 'data/msmarco/'
embedding: 'bert'
glove_embedding_path: 'data/embeddings/glove.6B.300d.txt'
glove_word2idx_path: 'small_data/glove.6B.300d.txt.word2idx_dict.p'
