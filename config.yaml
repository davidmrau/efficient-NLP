# choose which model to use {tf : Transformer (BERT-like) / snrm : SNRM}
model: 'snrm'
vocab_size: 400002
batch_size_train: 16
batch_size_test: 30
lr: 0.0001
stopwords: "none"
remove_unk: False
patience: 5
samples_per_epoch_train: 5000
samples_per_epoch_val: 20000
log_every_ratio: 0.01
sparse_dimensions: 1000
large_out_biases: False
num_workers: 0
bottleneck_run: False
bert:
  hidden_size: 768
  num_of_layers: 2
  num_attention_heads: 12
  input_length_limit: 512
  load_bert_layers: '0-1' # for multiple layers add '-' between hidden sizes  eg. ('0-1-4'). [0 are the embeddngs, 1 the first bert_layer etc.]
  load_bert_path: 'default'
  point_wise: True
tf:
  hidden_size: 256
  num_of_layers: 2
  num_attention_heads: 4
  input_length_limit: 150
  # the method that the hidden states over all sequence steps are being aggregated {CLS, AVG, MAX}
  pooling_method: 'AVG'
  layer_norm: True
  act_func: "relu"
  load_bert_layers: '' # for multiple layers add '-' between hidden sizes  eg. ('0-1-4'). [0 are the embeddngs, 1 the first bert_layer etc.]
  load_bert_path: 'default'
snrm:
  hidden_sizes: '200' # for multiple layers add '-' between hidden sizes  eg. ('100-400-200')
  n: 5
  dropout_p: 0.2
  n_gram_model: "cnn" # cnn or bert, defining the way that the n gram is being aggregated
  embedding_dim: 300
rank_model:
  hidden_sizes: '200' # for multiple layers add '-' between hidden sizes  eg. ('100-400-200')
  dropout_p: 0.2
  embedding_dim: 768
  weights: 'data/embeddings/idf_tensor.p' # a path to a pickled tensor OR "uniform" OR "random"
  trainable_weights: True
msmarco:
  max_query_len: 64
  max_complete_length: 512
robust04:
  max_length: 1500
debug: False
max_rank: 1000
experiments_dir: "experiments_${dataset}"
temp_exp_prefix: "Dev..."
num_epochs: 100
l1_scalar: 0.0
num_of_decimals: 5
balance_scalar: 0.0
disable_cuda: False
seed: 1
data_path: "data/"
embedding: 'bert'
bert_rel: False
bert_relevance_embeddings_path: '${data_path}/embeddings/bert_relev_embeddings.p'
glove_embedding_path: '${data_path}/embeddings/glove.6B.300d.p'
msmarco_query_test: '${data_path}/msmarco/msmarco-test2019-queries_43.tsv_${embedding}_stop_none_max_len_150.tsv'
msmarco_docs_test: '${data_path}/msmarco/msmarco-passagetest2019-top1000_43.tsv_${embedding}_stop_none_max_len_150.tsv'
msmarco_query_train: '${data_path}/msmarco/queries.train.tsv_${embedding}_stop_none_max_len_150.tsv'
msmarco_docs_train: '${data_path}/msmarco/collection.tsv_${embedding}_stop_none_max_len_150.tsv'
msmarco_triplets_train: '${data_path}/msmarco/qidpidtriples.train.full.tsv'
msmarco_triplets_train_debug: '${data_path}/msmarco/qidpidtriples.train.full.debug.tsv'
msmarco_ranking_results_test: '${data_path}/msmarco/msmarco-passagetest2019-top1000_43_ranking_results_style.tsv'
msmarco_qrel_test: '${data_path}/msmarco/2019qrels-pass.txt'
robust_ranking_results_train: "${data_path}/robust04/robust04_AOL_anserini_top_1000_qld_no_stem"
robust_ranking_results_train_debug: '${data_path}/robust04/robust04_AOL_anserini_top_1000_qld_no_stem_200k.filtered.debug.txt'
robust_docs: '${data_path}/robust04/robust04_raw_docs.num_query_${embedding}_stop_${stopwords}_remove_unk.tsv'
robust_query_train: '${data_path}/robust04/AOL-queries-all_filtered.txt.names_${embedding}_stop_${stopwords}_remove_unk_max_len_1500.tsv'
# robust_query_train: '${data_path}/robust04/AOL_queries_Replaced_non_Alphanumeric_With_space/AOL-queries-all_spaces_${embedding}_stop_${stopwords}_remove_unk.tsv'
robust_ranking_results_test: '${data_path}/robust04/robust04_TREC_test_anserini_top_2000_qld_ranking_results'
robust_query_test: '${data_path}/robust04/04.testset_num_query_lower_${embedding}_stop_${stopwords}_remove_unk.tsv'
robust_qrel_test: '${data_path}/robust04/qrels.robust2004.txt'
robust_ranking_results_strong: '${data_path}/robust04/qrels.robust2004.txt_pairwise'
robust_triplets_path: '${data_path}/robust04/weak_overfitting/all/test_top_2000.csv_TRIPLETS_top_100'
weak_min_results: 2
sampler: 'uniform'
target: 'binary'
top_k_per_query: 1000
samples_per_query: -1
dataset: ''
trec_eval: 'trec_eval'
folds_file: '${data_path}/robust04/robust04_strong_supervision_folds.p'
margin: 1
sample_j: False
single_sample: False
telegram: False
max_samples_per_gpu: -1
sample_random: False
weak_overfitting_test: False
validate_on_weak_test_results: False
provided_triplets: False
rerank_top_N: -1