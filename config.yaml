# choose which model to use {tf : Transformer (BERT-like) / snrm : SNRM}
model: 'snrm'
vocab_size: 30522
batch_size: 64
lr: 0.0001
sparse_dimensions: 10000
tf:
  hidden_size: 256
  num_of_layers: 2
  num_attention_heads: 4
  input_length_limit: 150
  # copy the embeddings from pretrained BERT (also affects hidden dim of model (784))
  pretrained_embeddings: True
  # the method that the hidden states over all sequence steps are being aggregated {CLS, AVG, MAX}
  pooling_method: 'CLS'
  # copy first N pretrained layers from BERT ( 0 for not using it !)
  use_N_pretrained_layers: 0
snrm:
  hidden_sizes: '200' # for multiple layers add '-' between hidden sizes  eg. ('100-400-200')
  n: 5
  dropout_p: 0.2
debug: False
num_epochs: 100
num_of_workers_index: 7
l1_scalar: 0.0
disable_cuda: False
dataset_path: 'data/msmarco/'
embedding: 'bert'
glove_embedding_path: 'data/embeddings/glove.6B.300d.txt'
