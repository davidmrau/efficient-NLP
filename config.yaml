# choose which model to use {tf : Transformer (BERT-like) / snrm : SNRM}
model: 'snrm'
vocab_size: 30522
batch_size: 64
lr: 0.0001
model_path: ''
sparse_dimensions: 10000
tf:
  hidden_size: 256
  num_of_layers: 2
  num_attention_heads: 4
  input_length_limit: 150
  # copy the embeddings from pretrained BERT (also affects hidden dim of model (784))
  pretrained_embeddings: False
  # the method that the hidden states over all sequence steps are being aggregated {CLS, AVG, MAX}
  pooling_method: 'CLS'
  # copy first N pretrained layers from BERT ( 0 for not using it !)
  use_N_pretrained_layers: 0
snrm:
  embedding_dim: 300
  hidden_sizes: '100-300'
  n: 5
  dropout_p: 0.2
debug: False
num_epochs: 20
num_of_workers_index: 7
l1_scalar: 0.0
disable_cuda: False
dataset_path: 'data/msmarco/'
embedding: 'glove'
glove_embedding_path: 'data/embeddings/glove.6B.300d.txt'
bert_embedding_path: 'data/embeddings/bert'
model_discr:
hydra:
  run:
    dir: experiments/model_${model}_l1_scalar_${l1_scalar}_lr_${lr}_drop_${snrm.dropout_p}_emb_${embedding}_batch_size_${batch_size}_debug_${debug}
