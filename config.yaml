# choose which model to use {tf : Transformer (BERT-like) / snrm : SNRM}
model: 'snrm'
vocab_size: 400002
batch_size_train: 256
batch_size_test: 256
lr: 0.0001
stopwords: "none"
remove_unk: False
patience: 5
samples_per_epoch_train: 5000
samples_per_epoch_val: 20000
log_every_ratio: 0.01
sparse_dimensions: 1000
large_out_biases: False
num_workers: 0
bottleneck_run: False
bert:
  hidden_size: 768
  num_of_layers: 2
  num_attention_heads: 12
  input_length_limit: 512
  load_bert_layers: '0-1' # for multiple layers add '-' between hidden sizes  eg. ('0-1-4'). [0 are the embeddngs, 1 the first bert_layer etc.]
  load_bert_path: 'default'
tf:
  hidden_size: 256
  num_of_layers: 2
  num_attention_heads: 4
  input_length_limit: 150
  # the method that the hidden states over all sequence steps are being aggregated {CLS, AVG, MAX}
  pooling_method: 'AVG'
  layer_norm: True
  act_func: "relu"
  load_bert_layers: '' # for multiple layers add '-' between hidden sizes  eg. ('0-1-4'). [0 are the embeddngs, 1 the first bert_layer etc.]
  load_bert_path: 'default'
snrm:
  hidden_sizes: '200' # for multiple layers add '-' between hidden sizes  eg. ('100-400-200')
  n: 5
  dropout_p: 0.2
  n_gram_model: "cnn" # cnn or bert, defining the way that the n gram is being aggregated
rank_model:
  hidden_sizes: '200' # for multiple layers add '-' between hidden sizes  eg. ('100-400-200')
  dropout_p: 0.1
  weights: 'data/embeddings/idf_tensor.p' # a path to a pickled tensor OR "uniform" OR "random"
  trainable_weights: True
  model_type: 'rank-interaction'
msmarco:
  max_query_len: 64
  max_complete_len: 512
robust04:
  max_len: 1500
  max_query_len: 64
  max_complete_len: 512
av_repr:
  #weights: 'data/embeddings/idf_tensor.p' # a path to a pickled tensor OR "uniform" OR "random"
  weights: 'uniform'
debug: False
experiments_dir: "experiments_${dataset}"
temp_exp_prefix: "Dev..."
num_epochs: 100
l1_scalar: 0.0
balance_scalar: 0.0
disable_cuda: False
seed: 1
data_path: "data/"
embedding: 'bert'
bert_rel: False
bert_relevance_embeddings_path: '${data_path}/embeddings/bert_relev_embeddings.p'
glove_embedding_path: '${data_path}/embeddings/glove.6B.300d.p'
word2vec_embedding_path: '${data_path}/embeddings/GoogleNews-vectors-negative300.p'

msmarco_query_test: '${data_path}/msmarco/msmarco-test2019-queries_43.tsv_${embedding}_stop_${stopwords}_remove_unk.tsv'
#msmarco_docs_test: '${data_path}/msmarco/msmarco-passagetest2019-top1000_43.tsv_${embedding}_stop_${stopwords}_remove_unk.tsv'
msmarco_docs_test: '${data_path}/msmarco/collection.tsv_${embedding}_stop_none_remove_unk.tsv'
msmarco_query_train: '${data_path}/msmarco/queries.train.tsv_${embedding}_stop_${stopwords}_remove_unk.tsv'

msmarco_docs_train: '${data_path}/msmarco/collection.tsv_${embedding}_stop_none_remove_unk.tsv'
msmarco_triplets: '${data_path}/msmarco/qidpidtriples.train.full.tsv'
msmarco_ranking_results_test: '${data_path}/msmarco/msmarco-passagetest2019-top1000_43_ranking_results_style.tsv'
msmarco_qrel_test: '${data_path}/msmarco/2019qrels-pass.txt'


robust_docs: '${data_path}/robust04/robust04_raw_docs.num_query_${embedding}_stop_${stopwords}_remove_unk.tsv'
robust_query_train: '${data_path}/robust04/aol_remove_test_10k.tsv_${embedding}_stop_${stopwords}_remove_unk.tsv'
robust_ranking_results_test: '${data_path}/robust04/robust04_anserini_TREC_test_top_2000_bm25'
robust_query_test: '${data_path}/robust04/04.testset_num_query_lower_${embedding}_stop_${stopwords}_remove_unk.tsv'
robust_qrel_test: '${data_path}/robust04/qrels.robust2004.txt'
robust_ranking_results_strong: '${data_path}/robust04/qrels.robust2004.txt_paper_binary_rand_docs'
robust_triples: '${data_path}/robust04/robust04_AOL_anserini_top_1000_bm25_10k_TRIPLETS_1000_shuf'
dataset: ''
trec_eval: 'trec_eval'
margin: 1
telegram: False
rerank_top_N: 2000
rand_p: 0
max_rank: 1000
